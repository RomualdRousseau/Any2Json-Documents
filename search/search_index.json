{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Any2Json Documents Revolutionizing Data Management: The Transformative Potential of a Novel Framework for Semi-Structured Documents Getting Started Tutorial 1 - Getting Started Tutorial 2 - Data extraction with a complex semi-structured layout Tutorial 3 - Data extraction with defects Tutorial 4 - Data extraction with tags Tutorial 5 - Data extraction with pivot Tutorial 6 - More complex noise reduction Tutorial 7 - Data extraction from PDF Tutorial 8 - Make a classifier from scratch How it works Please find detailed explanations on how Any2json works and its unique features here Plugins Any2Json Layex Parser Any2Json Next Classifier Any2Json Csv Any2Json Excel Any2Json Dbf Any2Json Parquet Models Models Resources White Papers","title":"Home"},{"location":"#welcome-to-any2json-documents","text":"Revolutionizing Data Management: The Transformative Potential of a Novel Framework for Semi-Structured Documents","title":"Welcome to Any2Json Documents"},{"location":"#getting-started","text":"Tutorial 1 - Getting Started Tutorial 2 - Data extraction with a complex semi-structured layout Tutorial 3 - Data extraction with defects Tutorial 4 - Data extraction with tags Tutorial 5 - Data extraction with pivot Tutorial 6 - More complex noise reduction Tutorial 7 - Data extraction from PDF Tutorial 8 - Make a classifier from scratch","title":"Getting Started"},{"location":"#how-it-works","text":"Please find detailed explanations on how Any2json works and its unique features here","title":"How it works"},{"location":"#plugins","text":"Any2Json Layex Parser Any2Json Next Classifier Any2Json Csv Any2Json Excel Any2Json Dbf Any2Json Parquet","title":"Plugins"},{"location":"#models","text":"Models","title":"Models"},{"location":"#resources","text":"White Papers","title":"Resources"},{"location":"how_it_works/","text":"Revolutionizing Data Management: The Transformative Potential of a Novel Framework for Semi-Structured Documents This document describes how Any2Json framework helps to manipulate semi-structured documents. In today's data-driven landscape, navigating the complexities of semi-structured documents poses a significant challenge for organizations. These documents, characterized by diverse formats and a lack of standardization, often require specialized skills for effective manipulation and analysis. However, we propose a novel framework to address this challenge. By leveraging innovative algorithms and machine learning techniques, this framework offers a solution that transcends manual coding, providing enhanced accessibility to users across diverse skill levels. Moreover, by automating the extraction process, it not only saves time but also minimizes errors, particularly beneficial for industries dealing with large volumes of such documents. Crucially, this framework integrates seamlessly with machine learning workflows, unlocking new possibilities for data enrichment and predictive modeling. Aligned with the paradigm of data as a service, it offers a scalable and efficient means of managing semi-structured data, thereby expanding the toolkit of data services available to organizations. This document highlights the transformative potential of the framework, paving the way for organizations to harness valuable insights from previously untapped sources and drive innovation in data management and analysis. Definitions and examples Semi-structured documents have the characteristic of containing some type of information known a priori, but which can change the position and format within the document itself. In addition, semi-structured documents also vary a lot in terms of layout and design. Some documents have a fixed set of data but no fixed format for this data. In some documents, the date appears on the top right corner, in another variation, it is at the center of the document, and in yet another, you\u2019ll find it in the bottom left corner. Another added complication is that the same data is qualified by different names. In one variation, a field may be called \u2018Purchase Order Number\u2019, in another - \u2018PO Number\u2019, and a few others may call it \u201cPO #\u201d, \u201cPO No.\u201d or \u201cOrder Number\u2019. These variations are endless and because of these two challenges, you cannot use a template-based solution for these documents. Examples Examples of semi-structured document include emails, XML files, JSON files, social media posts, and log files. These types of data contain both structured and unstructured information, such as a mixture of predefined fields & free-form text. Another classic example is the myriad of Excel files found within companies holding often precious information. What is the problems we are trying to fix? Semi-structured documents are usually noisy and their layout changes over time. They contain defects, usually invisible by the end user who read the document but provide a challenge to any automated process. They, also, contain tabular data but may be completed by unstructured text around. For examples, the date of the document or a product may be in the title and not in the tabular representation; simply extracting the table will miss some implicit information. These specifies stop today tool to consistently load such documents. The first consequence is a high running cost due to maintaining the pipeline of extraction every time the layout changed, due to the custom code required that need to be modified all the time. The second consequence is the difficulty to have a self-service to extract the data because Extract Load tools require often engineering skills (even for no/low code solution on the long run). The solution; a framework to manipulate semi-structured document and transform them into consistent tabular output Addressing Data Complexity: Semi-structured documents present a significant challenge due to their varied formats and lack of standardization. By developing algorithms and employing machine learning techniques, our framework can effectively handle this complexity without relying on manual coding. Enhanced Accessibility: Traditional solutions for working with semi-structured documents often require skilled engineers or developers. Our framework, however, eliminates this barrier by providing a user-friendly interface that doesn't require coding expertise. This makes it more accessible to a wider range of users within organizations. Increased Efficiency: Automating the process of extracting data from semi-structured documents not only saves time but also reduces the likelihood of errors that can occur with manual intervention. This efficiency gains can be particularly valuable in industries where large volumes of such documents need to be processed regularly. Integration with Machine Learning: By incorporating machine learning capabilities into the framework, you're not only extracting data but also enriching it with insights derived from advanced analytics. This opens up new possibilities for leveraging semi-structured data in machine learning workflows, enhancing decision-making and predictive modeling. Expansion of Data as a Service: Our framework aligns with the concept of data as a service by providing a scalable and efficient means of managing semi-structured data. This expands the toolbox of data services available to organizations, enabling them to harness valuable insights from previously untapped sources. A global overview of the framework The framework is actually written in Java and use several frameworks such as Tensorflow, Jython, etc \u2026 The framework loads and parses the document through different steps to extract and structure the data. The basic steps are: Load by Chunks The framework is able to load various format such as Excel, CSV, PDF, HTML \u2026 A plugin system allows to add new loader easily. Loaders stage the data into lists of chunks to allow processing of huge amount of data and minimize memory footprint between steps. Noise reduction Noise reduction consist to remove defects or visual artifacts such as: Empty cells used as row or column separator Empty cells inserted wrongly due to manual manipulations Merged cells \u2026 The noise reduction must be resilient to layout changes so simple removal of the nth column or row is not allowed. The framework includes the possibility to do small transformations using recipes. A recipe is a small Jython script that can access to the document model and manipulate it and perform basic cleansing. Feature Extraction Feature extraction will parse the cleaned document, detect tables and other meta data. It will build a tree of the different features of the document linked by their relation of importance. The relation of importance is based on the proximity of the elements to each other based on their reading direction. The human eye reads a document using a certain direction; from top to bottom and left to right for English or right to left depending in Arab. As such, when a human creates a document, he is influenced by this reading direction because he supposes his future reader to look at the document with a certain way. It means the various elements of the document will follow the reading direction and therefore can be linked to each other along this direction. You can find the detailed explanation in the white paper Semi-structured document feature extraction . Table Layout Parsing Feature extraction is not enough to structured the document, each table needs a deep analysis. The framework allows to parse the table to detect headers, sub headers, sub footers, footers, pivoted columns \u2026 A plugin system allows to add new parser into the framework. We will study an original approach to detect and extract the different parts of a table using a pattern matching algorithm similar to regular expressions. You can find the detailed explanation in the white paper Table Layout Regular Expression . Tabular Assembling This step will take all structures generated above and merge them into a single tabular structure. This structure is very easy to store in a database or a flat file such as parquet. You can find the detailed explanation in the white paper Semi-structured document feature extraction as we join a general algorithm to transform a semi-structured document to a tabular output. Tagging The last step will annotate each column of the tabular output a tag. Again a plugin system allows to add new tagger. This tag can be used as the name of the column when storing in a database. We will present in a tagger based on word embedding and a neural network to classify each data with an unique tag. Detailed Algorithm All these steps are summarized in the graph below: Conclusion Subsequent articles will be an in depth description of each steps of this framework and how to use it. All source codes are available on our github .","title":"How it works"},{"location":"how_it_works/#revolutionizing-data-management-the-transformative-potential-of-a-novel-framework-for-semi-structured-documents","text":"This document describes how Any2Json framework helps to manipulate semi-structured documents. In today's data-driven landscape, navigating the complexities of semi-structured documents poses a significant challenge for organizations. These documents, characterized by diverse formats and a lack of standardization, often require specialized skills for effective manipulation and analysis. However, we propose a novel framework to address this challenge. By leveraging innovative algorithms and machine learning techniques, this framework offers a solution that transcends manual coding, providing enhanced accessibility to users across diverse skill levels. Moreover, by automating the extraction process, it not only saves time but also minimizes errors, particularly beneficial for industries dealing with large volumes of such documents. Crucially, this framework integrates seamlessly with machine learning workflows, unlocking new possibilities for data enrichment and predictive modeling. Aligned with the paradigm of data as a service, it offers a scalable and efficient means of managing semi-structured data, thereby expanding the toolkit of data services available to organizations. This document highlights the transformative potential of the framework, paving the way for organizations to harness valuable insights from previously untapped sources and drive innovation in data management and analysis.","title":"Revolutionizing Data Management: The Transformative Potential of a Novel Framework for Semi-Structured Documents"},{"location":"how_it_works/#definitions-and-examples","text":"Semi-structured documents have the characteristic of containing some type of information known a priori, but which can change the position and format within the document itself. In addition, semi-structured documents also vary a lot in terms of layout and design. Some documents have a fixed set of data but no fixed format for this data. In some documents, the date appears on the top right corner, in another variation, it is at the center of the document, and in yet another, you\u2019ll find it in the bottom left corner. Another added complication is that the same data is qualified by different names. In one variation, a field may be called \u2018Purchase Order Number\u2019, in another - \u2018PO Number\u2019, and a few others may call it \u201cPO #\u201d, \u201cPO No.\u201d or \u201cOrder Number\u2019. These variations are endless and because of these two challenges, you cannot use a template-based solution for these documents. Examples Examples of semi-structured document include emails, XML files, JSON files, social media posts, and log files. These types of data contain both structured and unstructured information, such as a mixture of predefined fields & free-form text. Another classic example is the myriad of Excel files found within companies holding often precious information.","title":"Definitions and examples"},{"location":"how_it_works/#what-is-the-problems-we-are-trying-to-fix","text":"Semi-structured documents are usually noisy and their layout changes over time. They contain defects, usually invisible by the end user who read the document but provide a challenge to any automated process. They, also, contain tabular data but may be completed by unstructured text around. For examples, the date of the document or a product may be in the title and not in the tabular representation; simply extracting the table will miss some implicit information. These specifies stop today tool to consistently load such documents. The first consequence is a high running cost due to maintaining the pipeline of extraction every time the layout changed, due to the custom code required that need to be modified all the time. The second consequence is the difficulty to have a self-service to extract the data because Extract Load tools require often engineering skills (even for no/low code solution on the long run).","title":"What is the problems we are trying to fix?"},{"location":"how_it_works/#the-solution-a-framework-to-manipulate-semi-structured-document-and-transform-them-into-consistent-tabular-output","text":"Addressing Data Complexity: Semi-structured documents present a significant challenge due to their varied formats and lack of standardization. By developing algorithms and employing machine learning techniques, our framework can effectively handle this complexity without relying on manual coding. Enhanced Accessibility: Traditional solutions for working with semi-structured documents often require skilled engineers or developers. Our framework, however, eliminates this barrier by providing a user-friendly interface that doesn't require coding expertise. This makes it more accessible to a wider range of users within organizations. Increased Efficiency: Automating the process of extracting data from semi-structured documents not only saves time but also reduces the likelihood of errors that can occur with manual intervention. This efficiency gains can be particularly valuable in industries where large volumes of such documents need to be processed regularly. Integration with Machine Learning: By incorporating machine learning capabilities into the framework, you're not only extracting data but also enriching it with insights derived from advanced analytics. This opens up new possibilities for leveraging semi-structured data in machine learning workflows, enhancing decision-making and predictive modeling. Expansion of Data as a Service: Our framework aligns with the concept of data as a service by providing a scalable and efficient means of managing semi-structured data. This expands the toolbox of data services available to organizations, enabling them to harness valuable insights from previously untapped sources.","title":"The solution; a framework to manipulate semi-structured document and transform them into consistent tabular output"},{"location":"how_it_works/#a-global-overview-of-the-framework","text":"The framework is actually written in Java and use several frameworks such as Tensorflow, Jython, etc \u2026 The framework loads and parses the document through different steps to extract and structure the data. The basic steps are:","title":"A global overview of the framework"},{"location":"how_it_works/#load-by-chunks","text":"The framework is able to load various format such as Excel, CSV, PDF, HTML \u2026 A plugin system allows to add new loader easily. Loaders stage the data into lists of chunks to allow processing of huge amount of data and minimize memory footprint between steps.","title":"Load by Chunks"},{"location":"how_it_works/#noise-reduction","text":"Noise reduction consist to remove defects or visual artifacts such as: Empty cells used as row or column separator Empty cells inserted wrongly due to manual manipulations Merged cells \u2026 The noise reduction must be resilient to layout changes so simple removal of the nth column or row is not allowed. The framework includes the possibility to do small transformations using recipes. A recipe is a small Jython script that can access to the document model and manipulate it and perform basic cleansing.","title":"Noise reduction"},{"location":"how_it_works/#feature-extraction","text":"Feature extraction will parse the cleaned document, detect tables and other meta data. It will build a tree of the different features of the document linked by their relation of importance. The relation of importance is based on the proximity of the elements to each other based on their reading direction. The human eye reads a document using a certain direction; from top to bottom and left to right for English or right to left depending in Arab. As such, when a human creates a document, he is influenced by this reading direction because he supposes his future reader to look at the document with a certain way. It means the various elements of the document will follow the reading direction and therefore can be linked to each other along this direction. You can find the detailed explanation in the white paper Semi-structured document feature extraction .","title":"Feature Extraction"},{"location":"how_it_works/#table-layout-parsing","text":"Feature extraction is not enough to structured the document, each table needs a deep analysis. The framework allows to parse the table to detect headers, sub headers, sub footers, footers, pivoted columns \u2026 A plugin system allows to add new parser into the framework. We will study an original approach to detect and extract the different parts of a table using a pattern matching algorithm similar to regular expressions. You can find the detailed explanation in the white paper Table Layout Regular Expression .","title":"Table Layout Parsing"},{"location":"how_it_works/#tabular-assembling","text":"This step will take all structures generated above and merge them into a single tabular structure. This structure is very easy to store in a database or a flat file such as parquet. You can find the detailed explanation in the white paper Semi-structured document feature extraction as we join a general algorithm to transform a semi-structured document to a tabular output.","title":"Tabular Assembling"},{"location":"how_it_works/#tagging","text":"The last step will annotate each column of the tabular output a tag. Again a plugin system allows to add new tagger. This tag can be used as the name of the column when storing in a database. We will present in a tagger based on word embedding and a neural network to classify each data with an unique tag.","title":"Tagging"},{"location":"how_it_works/#detailed-algorithm","text":"All these steps are summarized in the graph below:","title":"Detailed Algorithm"},{"location":"how_it_works/#conclusion","text":"Subsequent articles will be an in depth description of each steps of this framework and how to use it. All source codes are available on our github .","title":"Conclusion"},{"location":"patents/","text":"Patents Patent 1 - Method to Consistently and Efficiently Extract Table Layout Feature This patent describes a method to build regular expression to consistently and efficiently extract data information from table. This method is based on the white paper Table Layout Regular Expression - Layex . Patent 2 - Semi-structured Document Feature Extraction This patent describes a method to build regular expression to consistently and efficiently extract features from document with unstructured (text) and semi-structured (table) elements. This method is based on the white paper Semi-structured Document Feature Extraction .","title":"Patents"},{"location":"patents/#patents","text":"","title":"Patents"},{"location":"patents/#patent-1-method-to-consistently-and-efficiently-extract-table-layout-feature","text":"This patent describes a method to build regular expression to consistently and efficiently extract data information from table. This method is based on the white paper Table Layout Regular Expression - Layex .","title":"Patent 1 - Method to Consistently and Efficiently Extract Table Layout Feature"},{"location":"patents/#patent-2-semi-structured-document-feature-extraction","text":"This patent describes a method to build regular expression to consistently and efficiently extract features from document with unstructured (text) and semi-structured (table) elements. This method is based on the white paper Semi-structured Document Feature Extraction .","title":"Patent 2 - Semi-structured Document Feature Extraction"},{"location":"tutorial_1/","text":"Tutorial 1 - Getting Started View source on GitHub . This short introduction uses Any2Json to: Load simple tabular documents in CSV and Exce formats. Display the tabular result on the console. The expected layout of each document is a single header row followed by rows of cells: Setup Any2Json Import the packages and setup the main class: package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial1 implements Runnable { public Tutorial1() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial1().run(); } } pom.xml Any2Json has a very modular design. Each module has to be loaded explicitely. The following modules are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency> Minimal code The minimal code to load a document is as follow: final var file = Common.loadData(f, this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\")) { doc.sheets().forEach(s -> s.getTable().ifPresent(t -> { doSomethingWithHeaders(t.headers()); doSomethingWithRows(t.rows()); })); } The encoding (\"UTF-8\" here) is used if the encoding could not be detected when loading the document. Iterate overs the headers: headers.forEach(h -> { // Do something with the header }); Iterate over the rows and cells: rows.forEach(r -> { r.cells().forEach(c -> { // Do something with the cell }); }); Load several file formats Here is a complete example to load and print the content of different CSV and Excel files: package com.github.romualdrousseau.any2json.examples; import java.util.List; import com.github.romualdrousseau.any2json.DocumentFactory; public class Tutorial1 implements Runnable { private static List<String> FILES = List.of( \"document with simple table.csv\", \"document with simple table.xls\", \"document with simple table.xlsx\"); public Tutorial1() { } @Override public void run() { FILES.forEach(f -> { final var file = Common.loadData(f, this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\")) { doc.sheets().forEach(s -> s.getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); } }); } public static void main(final String[] args) { new Tutorial1().run(); } } 2024-03-09 18:40:23 INFO Common:37 - Loaded resource: /data/document with simple table.csv Date Client Qty Amount 2023/02/01 AAA 1 100 2023/02/01 BBB 1 100 2023/02/01 BBB 3 300 2023/02/01 AAA 1 100 2024-03-09 18:40:24 INFO Common:37 - Loaded resource: /data/document with simple table.xls Date Client Qty Amount 2023-02-01 AAA 1 100 2023-02-01 BBB 1 100 2023-02-01 BBB 3 300 2023-02-01 AAA 1 100 2024-03-09 18:40:24 INFO Common:37 - Loaded resource: /data/document with simple table.xlsx Date Client Qty Amount 2023-02-01 AAA 1 100 2023-02-01 BBB 1 100 2023-02-01 BBB 3 300 2023-02-01 AAA 1 100 Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 1 - Getting Started"},{"location":"tutorial_1/#tutorial-1-getting-started","text":"View source on GitHub . This short introduction uses Any2Json to: Load simple tabular documents in CSV and Exce formats. Display the tabular result on the console. The expected layout of each document is a single header row followed by rows of cells:","title":"Tutorial 1 - Getting Started"},{"location":"tutorial_1/#setup-any2json","text":"","title":"Setup Any2Json"},{"location":"tutorial_1/#import-the-packages-and-setup-the-main-class","text":"package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial1 implements Runnable { public Tutorial1() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial1().run(); } }","title":"Import the packages and setup the main class:"},{"location":"tutorial_1/#pomxml","text":"Any2Json has a very modular design. Each module has to be loaded explicitely. The following modules are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency>","title":"pom.xml"},{"location":"tutorial_1/#minimal-code","text":"The minimal code to load a document is as follow: final var file = Common.loadData(f, this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\")) { doc.sheets().forEach(s -> s.getTable().ifPresent(t -> { doSomethingWithHeaders(t.headers()); doSomethingWithRows(t.rows()); })); } The encoding (\"UTF-8\" here) is used if the encoding could not be detected when loading the document.","title":"Minimal code"},{"location":"tutorial_1/#iterate-overs-the-headers","text":"headers.forEach(h -> { // Do something with the header });","title":"Iterate overs the headers:"},{"location":"tutorial_1/#iterate-over-the-rows-and-cells","text":"rows.forEach(r -> { r.cells().forEach(c -> { // Do something with the cell }); });","title":"Iterate over the rows and cells:"},{"location":"tutorial_1/#load-several-file-formats","text":"Here is a complete example to load and print the content of different CSV and Excel files: package com.github.romualdrousseau.any2json.examples; import java.util.List; import com.github.romualdrousseau.any2json.DocumentFactory; public class Tutorial1 implements Runnable { private static List<String> FILES = List.of( \"document with simple table.csv\", \"document with simple table.xls\", \"document with simple table.xlsx\"); public Tutorial1() { } @Override public void run() { FILES.forEach(f -> { final var file = Common.loadData(f, this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\")) { doc.sheets().forEach(s -> s.getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); } }); } public static void main(final String[] args) { new Tutorial1().run(); } } 2024-03-09 18:40:23 INFO Common:37 - Loaded resource: /data/document with simple table.csv Date Client Qty Amount 2023/02/01 AAA 1 100 2023/02/01 BBB 1 100 2023/02/01 BBB 3 300 2023/02/01 AAA 1 100 2024-03-09 18:40:24 INFO Common:37 - Loaded resource: /data/document with simple table.xls Date Client Qty Amount 2023-02-01 AAA 1 100 2023-02-01 BBB 1 100 2023-02-01 BBB 3 300 2023-02-01 AAA 1 100 2024-03-09 18:40:24 INFO Common:37 - Loaded resource: /data/document with simple table.xlsx Date Client Qty Amount 2023-02-01 AAA 1 100 2023-02-01 BBB 1 100 2023-02-01 BBB 3 300 2023-02-01 AAA 1 100","title":"Load several file formats"},{"location":"tutorial_1/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_2/","text":"Tutorial 2 - Data extraction with a complex semi-structured layout View source on GitHub . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here: Setup Any2Json Import the packages and setup the main class: package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial2 implements Runnable { public Tutorial2() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial2().run(); } } pom.xml Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-layex-parser\" module to enable the intelligent layout parsing. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency> Load base model To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser); Load the document We load the document by creating a document instance with the model and options to parse the document. The hint \"Document.Hint.INTELLI_LAYOUT\" will tell the document instance that the document has a complex layout. The recipe \"sheet.setCapillarityThreshold(0)\" will tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with multiple tables.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT)) .setRecipe(\"sheet.setCapillarityThreshold(0)\")) { ... } Output the tabular result Finally, we iterate over the sheets, rows and cells and outpout the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /data/document with multiple tables.xlsx 2024-03-09 18:58:43 DEBUG Common:59 - Extracting features ... 2024-03-09 18:58:43 DEBUG Common:63 - Generating Layout Graph ... 2024-03-09 18:58:43 DEBUG Common:67 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 18, 1, 18, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 19, 4, 24, 6, 4) (3) ================================== END ================================== 2024-03-09 18:58:43 DEBUG Common:72 - Done. A document very DATE PRODUCTNAME Client Qty Amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 2 - Data extraction with a complex semi-structured layout"},{"location":"tutorial_2/#tutorial-2-data-extraction-with-a-complex-semi-structured-layout","text":"View source on GitHub . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here:","title":"Tutorial 2 - Data extraction with a complex semi-structured layout"},{"location":"tutorial_2/#setup-any2json","text":"","title":"Setup Any2Json"},{"location":"tutorial_2/#import-the-packages-and-setup-the-main-class","text":"package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial2 implements Runnable { public Tutorial2() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial2().run(); } }","title":"Import the packages and setup the main class:"},{"location":"tutorial_2/#pomxml","text":"Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-layex-parser\" module to enable the intelligent layout parsing. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency>","title":"pom.xml"},{"location":"tutorial_2/#load-base-model","text":"To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser);","title":"Load base model"},{"location":"tutorial_2/#load-the-document","text":"We load the document by creating a document instance with the model and options to parse the document. The hint \"Document.Hint.INTELLI_LAYOUT\" will tell the document instance that the document has a complex layout. The recipe \"sheet.setCapillarityThreshold(0)\" will tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with multiple tables.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT)) .setRecipe(\"sheet.setCapillarityThreshold(0)\")) { ... }","title":"Load the document"},{"location":"tutorial_2/#output-the-tabular-result","text":"Finally, we iterate over the sheets, rows and cells and outpout the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /data/document with multiple tables.xlsx 2024-03-09 18:58:43 DEBUG Common:59 - Extracting features ... 2024-03-09 18:58:43 DEBUG Common:63 - Generating Layout Graph ... 2024-03-09 18:58:43 DEBUG Common:67 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 18, 1, 18, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 19, 4, 24, 6, 4) (3) ================================== END ================================== 2024-03-09 18:58:43 DEBUG Common:72 - Done. A document very DATE PRODUCTNAME Client Qty Amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form.","title":"Output the tabular result"},{"location":"tutorial_2/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_3/","text":"Tutorial 3 - Data extraction with defects View source on GitHub . This tutoral is a continuation of the Tutorial 2 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with defects. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here: Observe that there is hidden rows here and there. This kind of defect, mistakes or cosmetic artifacts, are very common and creates headaches when loading data. We will see that Any2Json automatically removes those artifacts (method called stiching) and determines if the data is part of a table or not. Setup Any2Json Import the packages and setup the main class: package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial3 implements Runnable { public Tutorial3() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial3().run(); } } pom.xml Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-layex-parser\" module to enable the intelligent layout parsing. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency> Load base model To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser); Load the document We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" will tell the document instance that the document has a complex layout. The recipe \"sheet.setCapillarityThreshold(0)\" will tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with defect.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT)) .setRecipe(\"sheet.setExtractionThreshold(0)\")) { ... } Output the tabular result Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /data/document with multiple tables.xlsx 2024-03-09 18:58:43 DEBUG Common:59 - Extracting features ... 2024-03-09 18:58:43 DEBUG Common:63 - Generating Layout Graph ... 2024-03-09 18:58:43 DEBUG Common:67 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 18, 1, 18, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 19, 4, 24, 6, 4) (3) ================================== END ================================== 2024-03-09 18:58:43 DEBUG Common:72 - Done. A document very DATE PRODUCTNAME Client Qty Amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe also how the artifacts didn't change the result and we didn't even need to change the code. Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 3 - Data extraction with defects"},{"location":"tutorial_3/#tutorial-3-data-extraction-with-defects","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 2 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with defects. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here: Observe that there is hidden rows here and there. This kind of defect, mistakes or cosmetic artifacts, are very common and creates headaches when loading data. We will see that Any2Json automatically removes those artifacts (method called stiching) and determines if the data is part of a table or not.","title":"Tutorial 3 - Data extraction with defects"},{"location":"tutorial_3/#setup-any2json","text":"","title":"Setup Any2Json"},{"location":"tutorial_3/#import-the-packages-and-setup-the-main-class","text":"package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial3 implements Runnable { public Tutorial3() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial3().run(); } }","title":"Import the packages and setup the main class:"},{"location":"tutorial_3/#pomxml","text":"Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-layex-parser\" module to enable the intelligent layout parsing. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency>","title":"pom.xml"},{"location":"tutorial_3/#load-base-model","text":"To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser);","title":"Load base model"},{"location":"tutorial_3/#load-the-document","text":"We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" will tell the document instance that the document has a complex layout. The recipe \"sheet.setCapillarityThreshold(0)\" will tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with defect.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT)) .setRecipe(\"sheet.setExtractionThreshold(0)\")) { ... }","title":"Load the document"},{"location":"tutorial_3/#output-the-tabular-result","text":"Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printHeaders(t.headers()); Common.printRows(t.rows()); })); 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 18:58:41 INFO Common:37 - Loaded resource: /data/document with multiple tables.xlsx 2024-03-09 18:58:43 DEBUG Common:59 - Extracting features ... 2024-03-09 18:58:43 DEBUG Common:63 - Generating Layout Graph ... 2024-03-09 18:58:43 DEBUG Common:67 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 18, 1, 18, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 19, 4, 24, 6, 4) (3) ================================== END ================================== 2024-03-09 18:58:43 DEBUG Common:72 - Done. A document very DATE PRODUCTNAME Client Qty Amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe also how the artifacts didn't change the result and we didn't even need to change the code.","title":"Output the tabular result"},{"location":"tutorial_3/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_4/","text":"Tutorial 4 - Data extraction with defects View source on GitHub . This tutoral is a continuation of the Tutorial 3 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with defects and its tagging capabilities. Tagging enable to fix a schema for the extracted data and ease the loading into a database for example. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here: Setup Any2Json Import the packages and setup the main class: package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial4 implements Runnable { public Tutorial4() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial4().run(); } } pom.xml Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-net-classifier\" module to enable the tagging capabilities. This module use TensorFlow for Java. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-net-classifier</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency> Load base model To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. Because we use the tagging capabilities in this tutorial, here are a subset of tags recognized by the base model: [ { \"name\" : \"tags\", \"doc\" : \"Tags recognized by sales-english model.\", \"settings\" : { \"types\" : [ \"none\", \"date\", \"dateYear\", \"dateMonth\", \"wholesalerCode\", \"wholesalerName\", \"customerCode\", \"customerName\", \"customerType\", \"customerGroup\", \"country\", \"postalCode\", \"adminArea1\", \"adminArea2\", \"adminArea3\", \"adminArea4\", \"locality\", \"address\", \"productCode\", \"productName\", \"amount\", \"unitPrice\", \"quantity\", \"bonusQuantity\", \"returnQuantity\", \"totalQuantity\", \"billToCode\", \"billToName\", \"transactionType\", \"invoiceNumber\", \"invoiceLineNumber\", \"batchNumber\", \"expiryDate\", \"creditReasonCode\", \"requesterName\" ], \"requiredTags\" : [ \"quantity\", \"productCode,productName\" ] } } ] The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser); Load the document We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" tell the document instance that the document has a complex layout. We also add the hint \"Document.Hint.INTELLI_TAG\" to tell that the tabular result must be tagged. The recipe \"sheet.setCapillarityThreshold(0)\" tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with defect.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT, Document.Hint.INTELLI_TAG)) .setRecipe(\"sheet.setCapillarityThreshold(0)\")) { ... } Output the tabular result Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printTags(t.headers()); Common.printRows(t.rows()); })); Note that now we are printing the tags of the headers and not their names. 2024-03-09 23:54:59 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 23:54:59 INFO Common:37 - Loaded resource: /data/document with defect.xlsx 2024-03-09 23:55:02 DEBUG Common:64 - Extracting features ... 2024-03-09 23:55:02 DEBUG Common:68 - Generating Layout Graph ... 2024-03-09 23:55:02 DEBUG Common:72 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 19, 1, 19, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 20, 4, 25, 6, 4) (3) ================================== END ================================== 2024-03-09 23:55:03.459511: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/model-9696004103989867291 2024-03-09 23:55:03.461712: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve } 2024-03-09 23:55:03.461749: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/model-9696004103989867291 2024-03-09 23:55:03.461804: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-03-09 23:55:03.477397: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 2024-03-09 23:55:03.478886: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2024-03-09 23:55:03.537380: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/model-9696004103989867291 2024-03-09 23:55:03.550411: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 90916 microseconds. 2024-03-09 23:55:03 DEBUG Common:77 - Done. none date productName customerName quantity amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe how the column names have been replaced by tags describing the recognized columns. Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 4 - Data extraction with defects"},{"location":"tutorial_4/#tutorial-4-data-extraction-with-defects","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 3 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with defects and its tagging capabilities. Tagging enable to fix a schema for the extracted data and ease the loading into a database for example. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here:","title":"Tutorial 4 - Data extraction with defects"},{"location":"tutorial_4/#setup-any2json","text":"","title":"Setup Any2Json"},{"location":"tutorial_4/#import-the-packages-and-setup-the-main-class","text":"package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial4 implements Runnable { public Tutorial4() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial4().run(); } }","title":"Import the packages and setup the main class:"},{"location":"tutorial_4/#pomxml","text":"Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-net-classifier\" module to enable the tagging capabilities. This module use TensorFlow for Java. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-net-classifier</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency>","title":"pom.xml"},{"location":"tutorial_4/#load-base-model","text":"To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. Because we use the tagging capabilities in this tutorial, here are a subset of tags recognized by the base model: [ { \"name\" : \"tags\", \"doc\" : \"Tags recognized by sales-english model.\", \"settings\" : { \"types\" : [ \"none\", \"date\", \"dateYear\", \"dateMonth\", \"wholesalerCode\", \"wholesalerName\", \"customerCode\", \"customerName\", \"customerType\", \"customerGroup\", \"country\", \"postalCode\", \"adminArea1\", \"adminArea2\", \"adminArea3\", \"adminArea4\", \"locality\", \"address\", \"productCode\", \"productName\", \"amount\", \"unitPrice\", \"quantity\", \"bonusQuantity\", \"returnQuantity\", \"totalQuantity\", \"billToCode\", \"billToName\", \"transactionType\", \"invoiceNumber\", \"invoiceLineNumber\", \"batchNumber\", \"expiryDate\", \"creditReasonCode\", \"requesterName\" ], \"requiredTags\" : [ \"quantity\", \"productCode,productName\" ] } } ] The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser);","title":"Load base model"},{"location":"tutorial_4/#load-the-document","text":"We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" tell the document instance that the document has a complex layout. We also add the hint \"Document.Hint.INTELLI_TAG\" to tell that the tabular result must be tagged. The recipe \"sheet.setCapillarityThreshold(0)\" tell the parser engine to extract the features as small as possible: final var file = Common.loadData(\"document with defect.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT, Document.Hint.INTELLI_TAG)) .setRecipe(\"sheet.setCapillarityThreshold(0)\")) { ... }","title":"Load the document"},{"location":"tutorial_4/#output-the-tabular-result","text":"Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printTags(t.headers()); Common.printRows(t.rows()); })); Note that now we are printing the tags of the headers and not their names. 2024-03-09 23:54:59 INFO Common:37 - Loaded resource: /models/sales-english.json 2024-03-09 23:54:59 INFO Common:37 - Loaded resource: /data/document with defect.xlsx 2024-03-09 23:55:02 DEBUG Common:64 - Extracting features ... 2024-03-09 23:55:02 DEBUG Common:68 - Generating Layout Graph ... 2024-03-09 23:55:02 DEBUG Common:72 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 4, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 5, 4, 10, 6, 4) (1) |- |- PRODUCTNAME META(1, 11, 1, 11, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 12, 4, 17, 6, 4) (2) |- |- PRODUCTNAME META(1, 19, 1, 19, 1, 1) |- |- |- Date Client Qty Amount DATA(1, 20, 4, 25, 6, 4) (3) ================================== END ================================== 2024-03-09 23:55:03.459511: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/model-9696004103989867291 2024-03-09 23:55:03.461712: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve } 2024-03-09 23:55:03.461749: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/model-9696004103989867291 2024-03-09 23:55:03.461804: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-03-09 23:55:03.477397: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 2024-03-09 23:55:03.478886: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2024-03-09 23:55:03.537380: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/model-9696004103989867291 2024-03-09 23:55:03.550411: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 90916 microseconds. 2024-03-09 23:55:03 DEBUG Common:77 - Done. none date productName customerName quantity amount A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 1ml BBB 1 100 A document very 2023-02-01 Product 1ml BBB 3 300 A document very 2023-02-01 Product 1ml AAA 1 100 A document very 2023-02-01 Product 2ml AAA 1 100 A document very 2023-02-01 Product 2ml BBB 2 200 A document very 2023-02-01 Product 2ml CCC 4 400 A document very 2023-02-01 Product 2ml DDD 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml CCC 1 100 A document very 2023-02-01 Product 3ml AAA 1 100 A document very 2023-02-01 Product 3ml DDD 1 100 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe how the column names have been replaced by tags describing the recognized columns.","title":"Output the tabular result"},{"location":"tutorial_4/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_5/","text":"Tutorial 5 - Data extraction with pivot View source on GitHub . This tutoral is a continuation of the Tutorial 4 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with pivot. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here: Setup Any2Json Import the packages and setup the main class: package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial5 implements Runnable { public Tutorial5() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial5().run(); } } pom.xml Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-net-classifier\" module to enable the tagging capabilities. This module use TensorFlow for Java. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-net-classifier</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency> Load base model To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.getPatternMap().put(\"(?i)((20|19)\\\\d{2}-(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)-\\\\d{2})\", \"DATE\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser); Load the document We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" tells the document instance that the document has a complex layout. We also add the hint \"Document.Hint.INTELLI_TAG\" to tell that the tabular result must be tagged. The recipe \"sheet.setCapillarityThreshold(0)\" tells the parser engine to extract the features as small as possible. The recipe \"sheet.setPivotOption(\\\"WITH_TYPE_AND_VALUE\\\")\" tells to manage the pivot: final var file = Common.loadData(\"document with pivot.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT, Document.Hint.INTELLI_TAG)) .setRecipe( \"sheet.setCapillarityThreshold(0)\", \"sheet.setPivotOption(\\\"WITH_TYPE_AND_VALUE\\\")\", \"sheet.setPivotTypeFormat(\\\"%s\\\")\")) { ... } Output the tabular result Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printTags(t.headers()); Common.printRows(t.rows()); })); 2024-03-11 20:03:41 INFO Common:42 - Loaded model: sales-english 2024-03-11 20:03:41 INFO Common:59 - Loaded resource: /data/document with pivot.xlsx 2024-03-11 20:03:44 DEBUG Common:86 - Extracting features ... 2024-03-11 20:03:44 DEBUG Common:90 - Generating Layout Graph ... 2024-03-11 20:03:44 DEBUG Common:94 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 7, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Client DATE #PIVOT? DATA(1, 5, 7, 11, 7, 4) (1) |- |- PRODUCTNAME META(1, 12, 1, 12, 1, 1) |- |- |- Client DATE #PIVOT? DATA(1, 13, 7, 19, 7, 4) (2) ================================== END ================================== 2024-03-11 20:03:44.868213: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/model-937345648011368689 2024-03-11 20:03:44.870396: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve } 2024-03-11 20:03:44.870431: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/model-937345648011368689 2024-03-11 20:03:44.870492: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-03-11 20:03:44.895354: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 2024-03-11 20:03:44.897818: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2024-03-11 20:03:44.978069: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/model-937345648011368689 2024-03-11 20:03:44.997561: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 129361 microseconds. 2024-03-11 20:03:45 DEBUG Common:99 - Done. none date productName customerName date amount quantity A document very 2023-Mar-02 Product 1ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Jan-01 300 3 A document very 2023-Mar-02 Product 1ml BBB 2023-Feb-01 300 3 A document very 2023-Mar-02 Product 1ml BBB 2023-Mar-02 300 3 A document very 2023-Mar-02 Product 1ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Jan-01 300 3 A document very 2023-Mar-02 Product 2ml BBB 2023-Feb-01 300 3 A document very 2023-Mar-02 Product 2ml BBB 2023-Mar-02 300 3 A document very 2023-Mar-02 Product 2ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Mar-02 100 1 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe how the date columns has been unpivoted. Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 5 - Data extraction with pivot"},{"location":"tutorial_5/#tutorial-5-data-extraction-with-pivot","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 4 . This tutorial will demonstrate how to use Any2Json to extract data from one Excel spreadsheet with pivot. To demonstrate the usage of this framework, we will load a document with a somewhat complex layout, as seen here:","title":"Tutorial 5 - Data extraction with pivot"},{"location":"tutorial_5/#setup-any2json","text":"","title":"Setup Any2Json"},{"location":"tutorial_5/#import-the-packages-and-setup-the-main-class","text":"package com.github.romualdrousseau.any2json.examples; import java.util.EnumSet; import java.util.List; import com.github.romualdrousseau.any2json.Document; import com.github.romualdrousseau.any2json.DocumentFactory; import com.github.romualdrousseau.any2json.parser.LayexTableParser; public class Tutorial5 implements Runnable { public Tutorial5() { } @Override public void run() { // Code will come here } public static void main(final String[] args) { new Tutorial5().run(); } }","title":"Import the packages and setup the main class:"},{"location":"tutorial_5/#pomxml","text":"Any2Json has a very modular design where each functionality can be loaded separatly. We add the \"any2json-net-classifier\" module to enable the tagging capabilities. This module use TensorFlow for Java. The following depedencies are required to run the code of this tutorial: <!-- ShuJu Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju</artifactId> <version>${shuju.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>shuju-jackson</artifactId> <version>${shuju.version}</version> </dependency> <!-- Any2Json Framework --> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-layex-parser</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-net-classifier</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-csv</artifactId> <version>${any2json.version}</version> </dependency> <dependency> <groupId>com.github.romualdrousseau</groupId> <artifactId>any2json-excel</artifactId> <version>${any2json.version}</version> </dependency>","title":"pom.xml"},{"location":"tutorial_5/#load-base-model","text":"To parse a document, any2Json needs a model that will contains the parameters required to the parsing. Instead to start from an empty Model (See Tutorial 7 ), we will start from an existing one and we will adapt it for our document. You can find a list and details of all models here . The base model, we will use, is \"sales-english\" that has been trained on 200+ english documents containing distributor data and with a large range of different layouts. The base model already recognize some entities such as DATE and NUMBER. We will setup the model to add one new entity PRODUCTNAME and we will configure a layex to extract the different elements of the documents. You can find more details about layex here . final var model = Common.loadModelFromGitHub(\"sales-english\"); // Add product name entity to the model model.getEntityList().add(\"PRODUCTNAME\"); model.getPatternMap().put(\"\\\\D+\\\\dml\", \"PRODUCTNAME\"); model.getPatternMap().put(\"(?i)((20|19)\\\\d{2}-(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)-\\\\d{2})\", \"DATE\"); model.update(); // Add a layex to the model final var tableParser = new LayexTableParser( List.of(\"(v.$)+\"), List.of(\"(()(S+$S+$))(()([/^TOTAL/|v].+$)())+(/TOTAL/.+$)\")); model.registerTableParser(tableParser);","title":"Load base model"},{"location":"tutorial_5/#load-the-document","text":"We load the document by creating a document instance with the model. The hint \"Document.Hint.INTELLI_LAYOUT\" tells the document instance that the document has a complex layout. We also add the hint \"Document.Hint.INTELLI_TAG\" to tell that the tabular result must be tagged. The recipe \"sheet.setCapillarityThreshold(0)\" tells the parser engine to extract the features as small as possible. The recipe \"sheet.setPivotOption(\\\"WITH_TYPE_AND_VALUE\\\")\" tells to manage the pivot: final var file = Common.loadData(\"document with pivot.xlsx\", this.getClass()); try (final var doc = DocumentFactory.createInstance(file, \"UTF-8\") .setModel(model) .setHints(EnumSet.of(Document.Hint.INTELLI_LAYOUT, Document.Hint.INTELLI_TAG)) .setRecipe( \"sheet.setCapillarityThreshold(0)\", \"sheet.setPivotOption(\\\"WITH_TYPE_AND_VALUE\\\")\", \"sheet.setPivotTypeFormat(\\\"%s\\\")\")) { ... }","title":"Load the document"},{"location":"tutorial_5/#output-the-tabular-result","text":"Finally, we iterate over the sheets, rows and cells and output the data on the console: doc.sheets().forEach(s -> Common.addSheetDebugger(s).getTable().ifPresent(t -> { Common.printTags(t.headers()); Common.printRows(t.rows()); })); 2024-03-11 20:03:41 INFO Common:42 - Loaded model: sales-english 2024-03-11 20:03:41 INFO Common:59 - Loaded resource: /data/document with pivot.xlsx 2024-03-11 20:03:44 DEBUG Common:86 - Extracting features ... 2024-03-11 20:03:44 DEBUG Common:90 - Generating Layout Graph ... 2024-03-11 20:03:44 DEBUG Common:94 - Assembling Tabular Output ... ============================== DUMP GRAPH =============================== Sheet1 |- A document very important DATE META(1, 1, 7, 1, 1, 1) |- |- PRODUCTNAME META(1, 4, 1, 4, 1, 1) |- |- |- Client DATE #PIVOT? DATA(1, 5, 7, 11, 7, 4) (1) |- |- PRODUCTNAME META(1, 12, 1, 12, 1, 1) |- |- |- Client DATE #PIVOT? DATA(1, 13, 7, 19, 7, 4) (2) ================================== END ================================== 2024-03-11 20:03:44.868213: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/model-937345648011368689 2024-03-11 20:03:44.870396: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve } 2024-03-11 20:03:44.870431: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/model-937345648011368689 2024-03-11 20:03:44.870492: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-03-11 20:03:44.895354: I external/org_tensorflow/tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled 2024-03-11 20:03:44.897818: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2024-03-11 20:03:44.978069: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/model-937345648011368689 2024-03-11 20:03:44.997561: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 129361 microseconds. 2024-03-11 20:03:45 DEBUG Common:99 - Done. none date productName customerName date amount quantity A document very 2023-Mar-02 Product 1ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 1ml BBB 2023-Jan-01 300 3 A document very 2023-Mar-02 Product 1ml BBB 2023-Feb-01 300 3 A document very 2023-Mar-02 Product 1ml BBB 2023-Mar-02 300 3 A document very 2023-Mar-02 Product 1ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 1ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Mar-02 100 1 A document very 2023-Mar-02 Product 2ml BBB 2023-Jan-01 300 3 A document very 2023-Mar-02 Product 2ml BBB 2023-Feb-01 300 3 A document very 2023-Mar-02 Product 2ml BBB 2023-Mar-02 300 3 A document very 2023-Mar-02 Product 2ml AAA 2023-Jan-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Feb-01 100 1 A document very 2023-Mar-02 Product 2ml AAA 2023-Mar-02 100 1 On this output, we print out the graph of the document built during the parsing and we can see clearly the relation between the elements of the spreadsheet and how there are structured in tabular form. Observe how the date columns has been unpivoted.","title":"Output the tabular result"},{"location":"tutorial_5/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_6/","text":"Tutorial 6 - More complex noise reduction View source on GitHub . This tutoral is a continuation of the Tutorial 5 . Coming soon Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 6 - More complex noise reduction"},{"location":"tutorial_6/#tutorial-6-more-complex-noise-reduction","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 5 . Coming soon","title":"Tutorial 6 - More complex noise reduction"},{"location":"tutorial_6/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_7/","text":"Tutorial 7 - Data extraction from PDF View source on GitHub . This tutoral is a continuation of the Tutorial 6 . Coming soon Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 7 - Data extraction from PDF"},{"location":"tutorial_7/#tutorial-7-data-extraction-from-pdf","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 6 . Coming soon","title":"Tutorial 7 - Data extraction from PDF"},{"location":"tutorial_7/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"tutorial_8/","text":"Tutorial 8 - Make a classifier from scratch View source on GitHub . This tutoral is a continuation of the Tutorial 7 . Coming soon Conclusion Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Tutorial 8 - Make a classifier from scratch"},{"location":"tutorial_8/#tutorial-8-make-a-classifier-from-scratch","text":"View source on GitHub . This tutoral is a continuation of the Tutorial 7 . Coming soon","title":"Tutorial 8 - Make a classifier from scratch"},{"location":"tutorial_8/#conclusion","text":"Congratulations! You have loaded documents using Any2Json. For more examples of using Any2Json, check out the tutorials .","title":"Conclusion"},{"location":"white_papers/","text":"White Papers Semi-structured Document Feature Extraction The document discusses the challenges organizations face in dealing with semi-structured documents, particularly spreadsheets, due to their diverse formats and lack of standardization. It highlights the presence of defects within spreadsheets, often unnoticed by end-users, which pose difficulties for automated processes. The document proposes a method to classify spreadsheet elements and create a structured format resembling a JSON file to address these challenges. Click here to open the white paper Table Layout Regular Expression - Layex In the modern landscape of data presentation, tables serve as a ubiquitous tool for organizing and conveying information efficiently. Whether in the structured presentation of scientific findings or the widespread use of spreadsheets in corporate environments, tables play a pivotal role in facilitating data interpretation. Consequently, the extraction of valuable insights encapsulated within these tables becomes paramount in any data pipeline process. This white paper introduces a novel mechanism designed to streamline the extraction of data from tables, particularly those with intricate layouts. Through the construction of a regular language customized to tabular representation, it aims to enhance efficiency and accuracy in data extraction processes, ultimately empowering organizations to unlock the full potential of their tabular data assets. Click here to open the white paper","title":"White Papers"},{"location":"white_papers/#white-papers","text":"","title":"White Papers"},{"location":"white_papers/#semi-structured-document-feature-extraction","text":"The document discusses the challenges organizations face in dealing with semi-structured documents, particularly spreadsheets, due to their diverse formats and lack of standardization. It highlights the presence of defects within spreadsheets, often unnoticed by end-users, which pose difficulties for automated processes. The document proposes a method to classify spreadsheet elements and create a structured format resembling a JSON file to address these challenges. Click here to open the white paper","title":"Semi-structured Document Feature Extraction"},{"location":"white_papers/#table-layout-regular-expression-layex","text":"In the modern landscape of data presentation, tables serve as a ubiquitous tool for organizing and conveying information efficiently. Whether in the structured presentation of scientific findings or the widespread use of spreadsheets in corporate environments, tables play a pivotal role in facilitating data interpretation. Consequently, the extraction of valuable insights encapsulated within these tables becomes paramount in any data pipeline process. This white paper introduces a novel mechanism designed to streamline the extraction of data from tables, particularly those with intricate layouts. Through the construction of a regular language customized to tabular representation, it aims to enhance efficiency and accuracy in data extraction processes, ultimately empowering organizations to unlock the full potential of their tabular data assets. Click here to open the white paper","title":"Table Layout Regular Expression - Layex"}]}